{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_TFP_SBC data preprocessing\n",
    "\n",
    "This notebook has been generated on 2020-04-18 07:59 \n",
    "\n",
    "The objective of this notebook is to compute TFP using OP or LP algorithm using ASIF panel data\n",
    "\n",
    "## Global steps \n",
    "\n",
    "The global steps to construct the dataset are the following:\n",
    "\n",
    "* Steps:\n",
    "  * Import data 2001-2007\n",
    "  * Select cities and industries from the paper's table\n",
    "  * Exclude outliers\n",
    "  * Remove firm with different:\n",
    "    *  ownership, cities and industries over time\n",
    "  * Compute TFP using 2 ways:\n",
    "    * full samples\n",
    "    * Split by ownership\n",
    "\n",
    "## Data source \n",
    "\n",
    "The data source to construct the dataset are the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Query Dataset \n",
    " \n",
    " - [SBC_pollution_China](https://console.cloud.google.com/bigquery?project=valid-pagoda-132423&p=valid-pagoda-132423&d=China&t=SBC_pollution_China&page=table) \n",
    " - [asif_firm_china](https://console.cloud.google.com/bigquery?project=valid-pagoda-132423&p=valid-pagoda-132423&d=China&t=asif_firm_china&page=table) \n",
    "### Google Cloud Storage Dataset \n",
    " \n",
    "### Google Spreadsheet Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination\n",
    "\n",
    "The new dataset is available from XXX\n",
    "\n",
    "- GS: None\n",
    "- GCS: 01_TFP_SBC.gz\n",
    "- BG: 01_TFP_SBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fast_connectCloud import connector\n",
    "from GoogleDrivePy.google_drive import connect_drive\n",
    "from GoogleDrivePy.google_platform import connect_cloud_platform\n",
    "from app_creation import studio\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas_profiling\n",
    "from pathlib import Path\n",
    "import os, re,  requests, json \n",
    "\n",
    "from dask.distributed import Client\n",
    "#from dask import dataframe as dd \n",
    "client = Client()  # set up local cluster on your laptop\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gs = connector.open_connection(online_connection = False, \n",
    "\tpath_credential = '/Users/thomas/Google Drive/Projects/Client_Oauth/Google_auth/')\n",
    "\n",
    "service_gd = gs.connect_remote(engine = 'GS')\n",
    "service_gcp = gs.connect_remote(engine = 'GCP')\n",
    "\n",
    "gdr = connect_drive.connect_drive(service_gd['GoogleDrive'])\n",
    "\n",
    "project = 'SBC_pollution_China'\n",
    "gcp = connect_cloud_platform.connect_console(project = project,\n",
    "\t\t\t\t\t\t\t\t\t\t\t service_account = service_gcp['GoogleCloudP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SBC_pollution_China from Google Big Query\n",
    "\n",
    "Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM China.SBC_pollution_China \"\n",
    "\n",
    "        )\n",
    "\n",
    "df_SBC_pollution_China = gcp.upload_data_from_bigquery(query = query,\n",
    "                                                       location = 'US',\n",
    "                                                      to_dask = True)\n",
    "df_SBC_pollution_China\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_SBC_pollution_China = dd.from_pandas(df_SBC_pollution_China, npartitions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load asif_firm_china from Google Big Query\n",
    "\n",
    "Feel free to add description about the dataset or any usefull information.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"\"\"\n",
    "WITH sum_id AS (\n",
    "  SELECT \n",
    "    id, \n",
    "    case WHEN ownership = 'SOE' THEN 'SOE' ELSE 'PRIVATE' END AS OWNERSHIP, \n",
    "    year, \n",
    "    geocode4_corr, \n",
    "    cic as industry, \n",
    "    SUM(output / 10000000) as output_agg_o, \n",
    "    SUM(fa_net / 10000000) as fa_net_agg_o, \n",
    "    SUM(employment / 100000) as employment_agg_o, \n",
    "    SUM(input / 10000000) as input_agg_o, \n",
    "  FROM \n",
    "    China.asif_firm_china \n",
    "  WHERE \n",
    "    year >= 2002 \n",
    "    AND year <= 2007 \n",
    "    AND output > 0 \n",
    "    AND fa_net > 0 \n",
    "    AND employment > 0 \n",
    "    AND input > 0 \n",
    "  GROUP BY \n",
    "    id, \n",
    "    OWNERSHIP, \n",
    "    year, \n",
    "    geocode4_corr, \n",
    "    cic, \n",
    "    OWNERSHIP\n",
    ") \n",
    "SELECT \n",
    "  sum_id.id, \n",
    "  OWNERSHIP, \n",
    "  year, \n",
    "  geocode4_corr, \n",
    "  industry, \n",
    "  output_agg_o, \n",
    "  fa_net_agg_o, \n",
    "  employment_agg_o, \n",
    "  input_agg_o, \n",
    "  occurence \n",
    "FROM \n",
    "  sum_id \n",
    "  LEFT JOIN (\n",
    "    SELECT \n",
    "      id, \n",
    "      COUNT(id) as occurence \n",
    "    FROM \n",
    "      sum_id \n",
    "    GROUP BY \n",
    "      id\n",
    "  ) as occ on sum_id.id = occ.id \n",
    "ORDER BY \n",
    "  occurence, \n",
    "  id, \n",
    "  year\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_asif_firm_china = gcp.upload_data_from_bigquery(query = query,\n",
    "                                                   location = 'US',\n",
    "                                                  to_dask =True)\n",
    "df_asif_firm_china.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asif_firm_china = dd.from_pandas(df_asif_firm_china, npartitions=3)\n",
    "df_asif_firm_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "In this section, we will construct the dataset, and document each step of the workflow.\n",
    "\n",
    "Please use the following format for the documentation:\n",
    "\n",
    "- Step 1: Select cities and industries from the paper's table\n",
    "    - (optional) Underlying process description\n",
    "- Step 2: Exclude outliers\n",
    "    - (optional) Underlying process description\n",
    "- Step 3: Remove firm with different:\n",
    "    - (optional) ownership, cities and industries over time\n",
    "- Step 4: Compute TFP\n",
    "    - Done with R in EC2\n",
    "- Step 5: AddT TFP to dataset\n",
    "    - Subset city characteristics\n",
    "        - Coastal\n",
    "        - TCZ\n",
    "        - Target\n",
    "    - Subset industry characteristic\n",
    "        - Polluted\n",
    "\n",
    "\n",
    "Note: **You need to rename the last dataframe `df_final`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = df_SBC_pollution_China['geocode4_corr'].unique().compute()\n",
    "industries = df_SBC_pollution_China['industry'].unique().compute()\n",
    "\n",
    "print(\"\"\"\n",
    "total cities : {}\\n\n",
    "total industries: {}\n",
    "\"\"\".format(len(cities), len(industries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_asif_firm_china.loc[\n",
    "                      (df_asif_firm_china['geocode4_corr'].isin(cities))&\n",
    "                      (df_asif_firm_china['industry'].isin(industries))\n",
    "                      ]\n",
    " .groupby([\"OWNERSHIP\",'occurence'])['occurence']\n",
    " .count()\n",
    " .compute()\n",
    " .unstack(0)\n",
    " .plot\n",
    " .bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers by `ONWERSHIP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = (df_asif_firm_china.loc[\n",
    "                      (df_asif_firm_china['geocode4_corr'].isin(cities))&\n",
    "                      (df_asif_firm_china['industry'].isin(industries))\n",
    "                      ]\n",
    " .groupby([\"OWNERSHIP\"])['output_agg_o']\n",
    " .apply(lambda x:x.quantile([.05,.25, .5, .75, .85, .9, .95]), meta=object)\n",
    " .compute()\n",
    " .unstack(0)\n",
    " .loc[[0.05, 0.95]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_asif_firm_china.loc[\n",
    "                      (df_asif_firm_china['geocode4_corr'].isin(cities))&\n",
    "                      (df_asif_firm_china['industry'].isin(industries))\n",
    "                      ]\n",
    " .groupby('OWNERSHIP')['output_agg_o']\n",
    " .apply(lambda x: x.describe(), meta=object)\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers#['SOE'].loc[0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_final = (\n",
    "    (df_asif_firm_china.loc[\n",
    "        (df_asif_firm_china['geocode4_corr'].isin(cities))&\n",
    "        (df_asif_firm_china['industry'].isin(industries)) &\n",
    "        (df_asif_firm_china['OWNERSHIP'].isin(['SOE'])) &\n",
    "        (df_asif_firm_china['output_agg_o']> outliers['SOE'].loc[0.05]) &\n",
    "        (df_asif_firm_china['output_agg_o']< outliers['SOE'].loc[0.95])\n",
    "                      ]\n",
    "     .append(\n",
    "        (\n",
    "        df_asif_firm_china.loc[\n",
    "        (df_asif_firm_china['geocode4_corr'].isin(cities))&\n",
    "        (df_asif_firm_china['industry'].isin(industries)) &\n",
    "        (df_asif_firm_china['OWNERSHIP'].isin(['PRIVATE'])) &\n",
    "        (df_asif_firm_china['output_agg_o']> outliers['PRIVATE'].loc[0.05]) &\n",
    "        (df_asif_firm_china['output_agg_o']< outliers['PRIVATE'].loc[0.95])\n",
    "     ]\n",
    "        )\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        switch_ownership = lambda x:\n",
    "        x.groupby(['id'])['OWNERSHIP'].transform('nunique', meta=object),\n",
    "        switch_cities = lambda x:\n",
    "        x.groupby('id')['geocode4_corr'].transform('nunique', meta=object),\n",
    "        switch_industry = lambda x:\n",
    "        x.groupby('id')['industry'].transform('nunique', meta=object)\n",
    "    )\n",
    "    #### Test if nan in the previous computation, if yes, and occurence is 1, \n",
    "    #### then 1\n",
    "    .assign(\n",
    "    switch_ownership = lambda x: \n",
    "        x['switch_ownership'].where(\n",
    "            ~x['switch_ownership'].isin([np.nan])& \n",
    "            ~x['occurence'].isin([1]),\n",
    "            1\n",
    "        ),\n",
    "    switch_cities = lambda x: \n",
    "        x['switch_cities'].where(\n",
    "            ~x['switch_cities'].isin([np.nan])& \n",
    "            ~x['occurence'].isin([1]),\n",
    "            1\n",
    "        ),\n",
    "    switch_industry = lambda x: \n",
    "        x['switch_industry'].where(\n",
    "            ~x['switch_industry'].isin([np.nan])& \n",
    "            ~x['occurence'].isin([1]),\n",
    "            1\n",
    "        ),\n",
    "    )\n",
    "    #### Create nan for switch\n",
    "    .assign(\n",
    "    switch_ownership = lambda x: \n",
    "        x['switch_ownership'].where(\n",
    "            x['switch_ownership'].isin([1]),\n",
    "            np.nan\n",
    "        ),\n",
    "    switch_cities = lambda x: \n",
    "        x['switch_cities'].where(\n",
    "            x['switch_cities'].isin([1]),\n",
    "            np.nan\n",
    "        ),\n",
    "    switch_industry = lambda x: \n",
    "        x['switch_industry'].where(\n",
    "            x['switch_industry'].isin([1]),\n",
    "            np.nan\n",
    "        ),\n",
    "    )\n",
    "    .dropna()\n",
    "    .reindex(columns = ['id',\n",
    "                        'occurence',\n",
    "                        'OWNERSHIP',\n",
    "                        'year',\n",
    "                        'geocode4_corr', \n",
    "                        'industry',\n",
    "                        'output_agg_o',\n",
    "                        'fa_net_agg_o',\n",
    "                        'employment_agg_o',\n",
    "                        'input_agg_o'])\n",
    ").compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_final.loc[lambda x: \n",
    "              x['OWNERSHIP'].isin(['SOE'])]\n",
    " .groupby(['year', 'occurence'])['output_agg_o']\n",
    " .describe()\n",
    " .sort_index(level = 0, ascending = False)\n",
    ")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First save locally\n",
    "df_final.to_csv(\n",
    "\t'01_TFP_SBC.gz',\n",
    "\tsep=',',\n",
    "\theader=True,\n",
    "\tindex=False,\n",
    "\tchunksize=100000,\n",
    "\tcompression='gzip',\n",
    "\tencoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: TFP computation\n",
    "\n",
    "There is an issue with the latest version of R and Mac. It makes it impossible to install library not available in Conda. So, we computed the TFP using an EC2 instance. \n",
    "\n",
    "Here are the related source files:\n",
    "\n",
    "- [Program](https://github.com/thomaspernet/SBC_pollution_China/blob/master/Data_preprocessing/program_tfp/tfp.R)\n",
    "- [Models](https://console.cloud.google.com/storage/browser/chinese_data/Panel_china/Asif_panel_china/TFP_computation)\n",
    "- [Data](https://storage.cloud.google.com/chinese_data/Panel_china/Asif_panel_china/TFP_computation/TFP_computed_ASIF_china_final.csv)\n",
    "\n",
    "Note, in R, need to concert `NaN` to `None`. BigQuery does not support `NaN`\n",
    "\n",
    "```\n",
    "(df.where(pd.notnull(df), None)\n",
    " .to_csv(\"program_tfp/TFP_computed_ASIF_china_final.csv\", index = False))\n",
    "```\n",
    "\n",
    "Three models:\n",
    "\n",
    "**Full sample**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1m9XCI9oXDbSZnKZfhj1rRbBNRyfmYfmF)\n",
    "\n",
    "**SOE sample**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1CUepeIMZINDoN63MX8LVORIZiNd5Sop6)\n",
    "\n",
    "**Private sample**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=10-QRGYUNOttZxtUucXfMttADOTay15Gt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "          \"SELECT * \"\n",
    "            \"FROM China.TFP_ASIF_china \"\n",
    "\n",
    "        )\n",
    "\n",
    "TFP_ASIF_china = gcp.upload_data_from_bigquery(query = query,\n",
    "                                                       location = 'US',\n",
    "                                                      to_dask = True)\n",
    "TFP_ASIF_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFP_ASIF_china[['tfp_OP', 'tfp_OWNERSHIP']].corr().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TFP_ASIF_china\n",
    ".groupby(['year','OWNERSHIP'])['tfp_OP']\n",
    ".mean() \n",
    ".compute() \n",
    ".unstack(-1) \n",
    ".plot\n",
    ".bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFP_ASIF_china.compute().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBC_TFP_ASIF_china = (TFP_ASIF_china\n",
    ".merge(\n",
    "    (df_SBC_pollution_China[[\n",
    "        'industry',\n",
    "        'polluted_thre'\n",
    "    ]]\n",
    "     .drop_duplicates(subset = 'industry')\n",
    "    ), on =  ['industry'])\n",
    ".merge(\n",
    "    (df_SBC_pollution_China[[\n",
    "    'geocode4_corr',\n",
    "    'cityen',    \n",
    "    'Coastal',\n",
    "    'TCZ_c',\n",
    "    'target_c'\n",
    "    ]]\n",
    " .drop_duplicates(subset = 'geocode4_corr')\n",
    "), on = ['geocode4_corr']\n",
    ")\n",
    "                      .drop(columns = \n",
    "                           ['output_agg_o',\n",
    "       'fa_net_agg_o', 'employment_agg_o', 'input_agg_o',\n",
    "                           'tfp_OP_soe', 'tfp_OP_pri', 'id_1',\n",
    "                           'switch_ownership',\n",
    "                            'switch_cities', 'switch_industry'])\n",
    ".assign(\n",
    "    #Period=lambda x: da.where(\n",
    "    #x[\"year\"] > 2005, \"Before\", \"After\"),\n",
    "        year=lambda x: x['year'].astype('str'),\n",
    "        industry=lambda x: x['industry'].astype('str')\n",
    "    )\n",
    ").compute().assign(\n",
    "    Period=lambda x: np.where(\n",
    "    x[\"year\"].isin(['2006', '2007']), \"After\", \"Before\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = SBC_TFP_ASIF_china.copy()\n",
    "df_final[\"FE_c_i\"] = pd.factorize(df_final[\"cityen\"] +\n",
    "                                      df_final['industry'])[0]\n",
    "\n",
    "df_final[\"FE_t_i\"] = pd.factorize(df_final[\"year\"] +\n",
    "                                      df_final['industry'])[0]\n",
    "\n",
    "df_final[\"FE_t_c\"] = pd.factorize(df_final[\"year\"] + df_final[\"cityen\"])[0]\n",
    "\n",
    "df_final[\"FE_c_i_o\"] = pd.factorize(df_final[\"cityen\"] + df_final[\"industry\"] +\n",
    "                                        df_final[\"OWNERSHIP\"])[0]\n",
    "df_final[\"FE_t_o\"] = pd.factorize(\n",
    "        df_final[\"year\"] + df_final[\"OWNERSHIP\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "\n",
    "In order to get a quick summary statistic of the data, we generate an HTML file with the profiling of the dataset we've just created. \n",
    "\n",
    "The profiling will be available at this URL after you commit a push to GitHub. \n",
    "\n",
    "**You need to rename the final dataframe `df_final` in the previous section to generate the profiling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### make sure the final dataframe is stored as df_final\n",
    "### Overide the default value: \n",
    "#https://github.com/pandas-profiling/pandas-profiling/blob/master/pandas_profiling/config_default.yaml\n",
    "\n",
    "profile = pandas_profiling.ProfileReport(df_final,\n",
    "                                        check_correlation_pearson = False)\n",
    "name_html = \"NAME.html\"\n",
    "profile.to_file(output_file=name_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to cloud\n",
    "\n",
    "The dataset is ready to be shared with your colleagues. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Move to GCS and BigQuery\n",
    "\n",
    "We move the dataset to the following:\n",
    "\n",
    "- **bucket**: *NEED TO DEFINE*\n",
    "\n",
    "- **Destination_blob**: *XXXXX/Processed_*\n",
    "- **name**:  *01_TFP_SBC.gz*\n",
    "- **Dataset**: *China*\n",
    "\n",
    "- **table**: *01_TFP_SBC*\n",
    "\n",
    "### GCS\n",
    "\n",
    "We first need to save *01_TFP_SBC* with `.gz` extension locally then we can move it\n",
    "to GCS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### First save locally\n",
    "df_final.to_csv(\n",
    "\t'01_TFP_SBC_firm.gz',\n",
    "\tsep=',',\n",
    "\theader=True,\n",
    "\tindex=False,\n",
    "\tchunksize=100000,\n",
    "\tcompression='gzip',\n",
    "\tencoding='utf-8')\n",
    "\n",
    "### Then upload to GCS\n",
    "bucket_name = 'chinese_data'\n",
    "destination_blob_name = 'paper_project/Processed_'\n",
    "source_file_name = '01_TFP_SBC_firm.gz'\n",
    "gcp.upload_blob(bucket_name, destination_blob_name, source_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Move to bigquery\n",
    "bucket_gcs ='chinese_data/paper_project/Processed/01_TFP_SBC_firm.gz'\n",
    "gcp.move_to_bq_autodetect(dataset_name= 'China',\n",
    "\t\t\t\t\t\t\t name_table= 'TFP_SBC_firm',\n",
    "\t\t\t\t\t\t\t bucket_gcs=bucket_gcs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Studio\n",
    "\n",
    "To generate a notebook ready to use in the studio, please fill in the variables below:\n",
    "\n",
    "- 'project_name' : Name of the repository\n",
    "- 'input_datasets' : name of the table\n",
    "- 'sheetnames' : Name of the sheet, if table saved in Google Spreadsheet\n",
    "- 'bigquery_dataset' : Dataset name\n",
    "- 'destination_engine' : 'GCP' or 'GS,\n",
    "- 'path_destination_studio' : path to `Notebooks_Ready_to_use_studio`\n",
    "- 'project' : 'valid-pagoda-132423',\n",
    "- 'username' : \"thomas\",\n",
    "- 'pathtoken' : Path to GCP token,\n",
    "- 'connector' : 'GBQ', ## change to GS if spreadsheet\n",
    "- 'labels' : Add any labels to the variables,\n",
    "- 'date_var' : Date variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "date_var = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"(.*)/(.*)\"\n",
    "path = os.getcwd()\n",
    "parent_path = Path(path).parent\n",
    "test_str = str(parent_path)\n",
    "matches = re.search(regex, test_str)\n",
    "github_repo = matches.group(2)\n",
    "\n",
    "path_credential = '/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/'\n",
    "\n",
    "dic_ = {\n",
    "    \n",
    "          'project_name' : github_repo,\n",
    "          'input_datasets' : 'TFP_SBC_firm',\n",
    "          'sheetnames' : '',\n",
    "          'bigquery_dataset' : 'China',\n",
    "          'destination_engine' : 'GCP',\n",
    "          'path_destination_studio' : os.path.join(test_str,\n",
    "                                       'Notebooks_Ready_to_use_studio'),\n",
    "          'project' : 'valid-pagoda-132423',\n",
    "          'username' : \"thomas\",\n",
    "          'pathtoken' : path_credential,\n",
    "          'connector' : 'GBQ', ## change to GS if spreadsheet\n",
    "          'labels' : labels,\n",
    "          'date_var' : date_var\n",
    "}\n",
    "create_studio = studio.connector_notebook(dic_)\n",
    "create_studio.generate_notebook_studio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to catalogue\n",
    "\n",
    "Now that the dataset is ready, you need to add the underlying information to the data catalogue. The data catalogue is stored in [Coda](https://coda.io/d/MasterFile-Database_dvfMWDBnHh8/MetaDatabase_suYFO#_ludIZ), more precisely, in the table named `DataSource`. \n",
    "\n",
    "The cells below helps you to push the information directly to the table using Coda API.\n",
    "\n",
    "The columns are as follow:\n",
    "\n",
    "- `Storage`: Define the location of the table\n",
    "    - GBQ, GS, MongoDB\n",
    "- `Theme`: Define a theme attached to the table\n",
    "    - Accountancy, Complexity, Correspondance, Customer_prediction, Distance, Environment, Finance, Macro, Production, Productivity, Survey, Trade\n",
    "- `Database`: Name of the dataset. Use only for GBQ or MongoDB (collection)\n",
    "    - Business, China, Steamforged, Trade\n",
    "- `Path`:A URL with the path of the location of the dataset\n",
    "- `Filename`: Name of the table\n",
    "- `Description`: Description of the table. Be very specific. \n",
    "- `Source_data`: A list of the data sources used to construct the table.\n",
    "- `Link_methodology`: URL linked to the notebook\n",
    "- `Dataset_documentation`: Github repository attached to the table\n",
    "- `Status`: Status of the table. \n",
    "    - `Closed` if the table won't be altered in the future\n",
    "    - `Active` if the table will be altered in the future\n",
    "- `Profiling`: Specify if the user created a Pandas profiling\n",
    "    - `True` if the profiling has been created\n",
    "    - `False` otherwise\n",
    "- `Profiling_URL`: Profiling URL (link to Github). Always located in `Data_catalogue/table_profiling`\n",
    "- `JupyterStudio`: Specify if the user created a notebook to open the studio\n",
    "    - `True` if the notebook has been created\n",
    "    - `False` otherwise\n",
    "- `JupyterStudio_launcher`: Notebook URL (link to Github). Always located in `Notebooks_Ready_to_use_studio`\n",
    "- `Nb_projects`: Number of projects using this dataset. A Coda formula. Do not update this row\n",
    "- `Created on`: Date of creation. A Coda formula. Do not update this row\n",
    "\n",
    "Remember to commit in GitHub to activate the URL link for the profiling and Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Storage = 'GBQ'\n",
    "Theme = 'Trade' \n",
    "Database = 'China'\n",
    "Description = \"The table is related to the paper about FTP and SBC\"\n",
    "Filename = 'TFP_SBC_firm'\n",
    "Status = 'Active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_data = ['SBC_pollution_China', 'asif_firm_china', 'TFP_ASIF_china']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell pushes the information to [Coda](https://coda.io/d/MasterFile-Database_dvfMWDBnHh8/Test-API_suDBp#API_tuDK4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"(.*)/(.*)\"\n",
    "path = os.getcwd()\n",
    "parent_path = Path(path).parent\n",
    "test_str = str(parent_path)\n",
    "matches = re.search(regex, test_str)\n",
    "github_repo = matches.group(2)\n",
    "\n",
    "Profiling = True\n",
    "if Profiling:\n",
    "    Profiling_URL = 'http://htmlpreview.github.io/?https://github.com/' \\\n",
    "    'thomaspernet/{}/blob/master/Data_catalogue/table_profiling/{}.html'.format(github_repo,\n",
    "                                                                               Filename)\n",
    "else:\n",
    "    Profiling_URL = ''\n",
    "JupyterStudio = False\n",
    "if JupyterStudio:\n",
    "    JupyterStudio_URL = '\"https://mybinder.org/v2/gh/thomaspernet/{0}/' \\\n",
    "    'master?filepath=Notebooks_Ready_to_use_studio%2F{1}_studio.ipynb'.format(github_repo, Filename)\n",
    "else:\n",
    "    JupyterStudio_URL = ''\n",
    "### BigQuery only \n",
    "path_url = 'https://console.cloud.google.com/bigquery?project=valid-pagoda-132423' \\\n",
    "'&p=valid-pagoda-132423&d=China&t={}&page=table'.format(Filename)\n",
    "\n",
    "Link_methodology = 'https://nbviewer.jupyter.org/github/thomaspernet/' \\\n",
    "    '{0}/blob/master/Data_preprocessing/' \\\n",
    "    '{1}.ipynb'.format(github_repo,\n",
    "    Filename)\n",
    "\n",
    "Dataset_documentation = 'https://github.com/thomaspernet/{}'.format(github_repo)\n",
    "\n",
    "to_add = {\n",
    "    'Storage': Storage,\n",
    "    'Theme': Theme,\n",
    "    'Database': Database,\n",
    "    'Path_url': path_url,\n",
    "    'Filename': Filename,\n",
    "    'Description': Description,\n",
    "    'Source_data': Source_data,\n",
    "    'Link_methodology': Link_methodology,\n",
    "    'Dataset_documentation': Dataset_documentation,\n",
    "    'Status': Status,\n",
    "    'Profiling_URL': Profiling_URL,\n",
    "    'JupyterStudio_launcher': JupyterStudio_URL\n",
    "\n",
    "}\n",
    "cols= []\n",
    "for key, value in to_add.items():\n",
    "    coda = {\n",
    "    'column': key,\n",
    "    'value':value\n",
    "    }\n",
    "    cols.append(coda)\n",
    "    \n",
    "###load token coda\n",
    "with open('token_coda.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "token = data[0]['token'] \n",
    "headers = {'Authorization': 'Bearer {}'.format(token)}\n",
    "uri = f'https://coda.io/apis/v1beta1/docs/vfMWDBnHh8/tables/grid-HgpAnIEhpP/rows'\n",
    "payload = {\n",
    "  'rows': [\n",
    "    {\n",
    "      'cells': cols,\n",
    "    },\n",
    "  ],\n",
    "}\n",
    "req = requests.post(uri, headers=headers, json=payload)\n",
    "req.raise_for_status() # Throw if there was an error.\n",
    "res = req.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
